{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InvertedBellCurve.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33B3zCxM-92O"
      },
      "source": [
        "# Inverted Bell-Curve based Ensemble of Deep Learning Models for Detection of COVID-19 from Chest X-rays\n",
        "\n",
        "Novel Coronavirus 2019 disease or COVID-19 is a viral disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Use of Chest X-Rays (CXRs) has become an important practice to assist in the diagnosis of COVID-19 as they can be used to detect the abnormalities developed in the infected patients' lungs. With the fast spread of the disease, many researchers across the world are striving to use several deep learning based systems to identify the COVID-19 from such CXR images. To this end, we propose an Inverted Bell-curve based ensemble of deep learning models for detection of COVID-19 from CXR images. We first use a selection of models pretrained on ImageNet dataset and use the concept of transfer learning to retrain them with CXR datasets. Then the trained models are combined with the proposed Inverted Bell Curve weighted ensemble method, where the output of each classifier is assigned a weight, and the final prediction is done by performing weighted average of those outputs. We evaluate the proposed method on two publicly available datasets: the COVID-19 Radiography Database and the IEEE COVID Chest X-Ray Dataset. The accuracy, F1 score and the AUC ROC achieved by the proposed method are 99.66\\%, 99.75\\% and 99.99\\% respectively in the first dataset, and 99.84\\%, 99.81\\%, 99.99\\% respectively in the other dataset. Experimental results ensure that the use of transfer learning based models and their combination using the proposed ensemble method result in improved predictions of COVID-19 in CXRs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTXKSsna-5U0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from  numpy import exp,absolute\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "\n",
        "\n",
        "path = '/content/drive/My Drive/COVIDxA/train/'\n",
        "\n",
        "plt.ion()   \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(path,transform=data_transforms)\n",
        "\n",
        "val_split = 0.8\n",
        "train_size = math.floor(len(dataset)*val_split)\n",
        "val_size = len(dataset) - train_size\n",
        "trainset, valset = data.random_split(dataset,lengths=[train_size,val_size])\n",
        "dataset_sizes = {'train':train_size,'val':val_size}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': data.DataLoader(trainset,batch_size=16,shuffle=True),\n",
        "    'val' : data.DataLoader(valset,batch_size=16,shuffle=True)\n",
        "}\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "class_names = dataset.classes\n",
        "num_classes = len(class_names)\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                print('bruh')\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgoiqK4o_2OK"
      },
      "source": [
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "model_vgg = torchvision.models.vgg16(pretrained=True)\n",
        "model_resnet = torchvision.models.resnet18(pretrained=True)\n",
        "model_alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "model_dnet = torchvision.models.densenet161(pretrained=True)\n",
        "for param in model_alexnet.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_vgg.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_resnet.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model_dnet.parameters():\n",
        "    param.requires_grad = True\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_resnet.fc.in_features\n",
        "model_resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
        "model_vgg.classifier[6] = nn.Linear(4096,num_classes)\n",
        "model_alexnet.classifier[6] = nn.Linear(4096,num_classes)\n",
        "model_dnet.classifier = nn.Linear(2208,num_classes)\n",
        "\n",
        "model_vgg = model_vgg.to(device)\n",
        "model_resnet = model_resnet.to(device)\n",
        "model_alexnet = model_alexnet.to(device)\n",
        "model_dnet = model_dnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_conv = optim.SGD(model_resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=70, gamma=0.1)\n",
        "model_resnet = train_model(model_resnet, criterion, optimizer_conv, exp_lr_scheduler,num_epochs=30)\n",
        "torch.save(model_resnet, '/content/drive/My Drive/kaggle_resnet.pth')\n",
        "\n",
        "optimizer_conv = optim.SGD(model_vgg.parameters(), lr=0.001, momentum=0.9)\n",
        "model_vgg = train_model(model_vgg, criterion, optimizer_conv, exp_lr_scheduler,num_epochs=20)\n",
        "torch.save(model_vgg, '/content/drive/My Drive/kaggle_vgg.pth')\n",
        "\n",
        "\n",
        "optimizer_conv = optim.SGD(model_dnet.parameters(), lr=0.001, momentum=0.9)\n",
        "model_dnet = train_model(model_dnet, criterion, optimizer_conv, exp_lr_scheduler,num_epochs=20)\n",
        "torch.save(model_dnet, '/content/drive/My Drive/kaggle_dense.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEB1kDNDAjmb"
      },
      "source": [
        "Let there be $C$ classes in dataset and $k$ classifiers trained on the dataset. In this paper value of $k$ is taken is 3 but $k$ can take any finite value. Let $s_i^j$ be the confidence score for $j^{th}$ class predicted by the $i^{th}$ classifier. The confidence scores are the output of softmax, hence output of some $i^{th}$ classifier will follow:\n",
        "\\begin{equation}\n",
        "    \\sum_{j=0}^C s_i^j = 1 \\quad\\quad {where}\\quad s_i^j\\space \\in \\space[0,1]\n",
        "\\end{equation}\n",
        "\n",
        "Now weight is assigned to each of the classifiers output using inverted Bell curve function which is a function in form of\n",
        "\\begin{equation}\n",
        "    f(x) = \\frac{1}{a} exp(\\frac{(x-b)^2}{2c^2})\n",
        "\\end{equation}\n",
        "\n",
        "The function $f(x)$ is also known as inverted Bell curve. The inverted bell-shape is particularly useful to implement this weighted averaging scheme. It can be observe that shape of $f(x)$ is more round at the bottom than any equivalent parabolic curve. We hypothesise that this helps in penalising a wider range of low confidence score values, resulting in a better ensemble.\n",
        "\n",
        "The parameter $a$ is inversly proportional to the depth of the inverted bell. The value of $a$ gets closer to $0$ the bottom of the curve comes nearer to the $x$-axis. The parameter $b$ controls the position of the centre of the curve bottom. At $x=b$, we can achieve the minimum value of $f(x)$ where $a > 0$. The parameter $c$ determines the width of the bell.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://user-images.githubusercontent.com/31564734/121768236-daaab000-cb7a-11eb-9a29-07dc69c9c56f.jpg\" width=\"500px\"/>\n",
        "</p>\n",
        "\n",
        "Let us consider the point $x=b$, where $f(x)$ has its minima given $a > 0$, so as $x$ is incremented or decremented we will get higher values of $f(x)$, similar amount at both direction due to the fact that $(x-b)$ term is squared in the equation. This very idea is used in the context of assigning weights to the outputs of each classifiers.\n",
        "\n",
        "Let us consider two independent classifiers $P$ and $Q$ produce $[0.8,0.1,0.1]$ and $[0.5,0.3,0.2]$ as output confidence scores for some input $X$. Though, both of these classifiers predict the $X$ belongs to class-0, the classifier $P$ does it more confidently. Therefore, while doing weighted average of these scores, we must assign more weight to the classifier $P$ for this output. In doing so, the property of $f(x)$ discussed above is used. Let the minima of $f(x)$ is at $x=0.5$, then we will get higher values of $f(x)$ as we get closer to 0 or 1 because these are respectively lower and upper bounds for $s_i^j$. It can be easily shown that minima of $f(x)$ exists at $x=b$. So the value of $b$ is taken as 0.5 to satisfy our requirement. The value of $c$ determines the range of the weights and it is chosen as 0.5 experimentally .\n",
        "\n",
        "\n",
        "There may arrive a situation when some classifiers having very poor performance metrics over a dataset but for some instances it produces the outputs confidently. Therefore, without suppressing the classifiers' impacts completely, we aim to weaken its contribution in the ensemble output, and we consider the accuracy of the classifier by taking $a=1/A_i.$ in $f(x)$, where $A_i$ is the accuracy for the $i^{th}$ classifier. So the wight $w_i$ assigned to the output of $i^{th}$ classifier is \n",
        "\n",
        "\\begin{equation}\n",
        "    w_i = A_i\\cdot\\sum_{j=0}^C f(s_i^j) \\quad where\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    f(x) = exp(\\frac{(x - 0.5)^2}{0.5})\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The final output $[Y_0, Y_1, ..., Y_C]$ is generated by taking the weighted average of confidence scores across $k$ classifiers using $w_i$, where\n",
        "\\begin{equation}\n",
        "    Y_j = \\frac{1}{k}\\cdot \\sum_{i=0}^k w_i\\cdot s_i^j\n",
        "\\end{equation}\n",
        "\n",
        "We can further apply softmax on the calculated $Y$ to normalise the output scores and obtain the final class probability. Finally the class $\\zeta$ is predicted from this output as\n",
        "\\begin{equation}\n",
        "    \\zeta = {arg max}_{j} \\{Y_j\\}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI-rQmbjBMN5"
      },
      "source": [
        "def ensemble(op_classifiers):\n",
        "    cf = 0.0\n",
        "    for c in op_classifiers:\n",
        "        c = c.cpu().numpy()\n",
        "        w = np.identity(c.shape[0])\n",
        "        i = 0\n",
        "        for x in c:\n",
        "            w[i][i] = np.sum(exp((x-0.5)*(x-0.5)/0.5))\n",
        "            i += 1\n",
        "        cf += np.matmul(w,c)\n",
        "    cf = (cf)/len(op_classifiers)\n",
        "    return torch.from_numpy(cf)\n",
        "\n",
        "soft = nn.Softmax(dim=1)\n",
        "running_corrects = 0\n",
        "\n",
        "for inputs, labels in dataloaders['val']:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        outputs_resnet = soft(model_resnet(inputs))\n",
        "        outputs_vgg = soft(model_vgg(inputs))\n",
        "        outputs_dnet = soft(model_dnet(inputs))\n",
        "        outputs_alexnet = soft(model_alexnet(inputs))\n",
        "        outputs = ensemble({outputs_dnet,outputs_resnet,outputs_vgg})\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "print(running_corrects/dataset_sizes['val'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}